{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUSINESS UNDERSTANDING\n",
    "\n",
    "Overview\n",
    "Kenya Power and Lighting Company (KPLC) often receives a high volume of tweets from customers reporting issues, asking questions, or providing feedback.Understanding customer sentiment towards KPLC is crucial to enable automating of responses, enhancing customer service efficiency, improving response times, and reduce the manual workload on customer service teams. The goal is to develop a chatbot capable of classifying various types of tweets and generating appropriate automated responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "KPLC needs an automated sentiment analysis system to process and categorize customer feedback from social media, particularly X formerly (Twitter) where customers frequently express their sentiments regarding KPLC's services. By accurately classifying tweets related to KPLC’s services into sentiment categories the system will be able to identify issues by pinpointing common complaints and service issues and enhance customer feedback\n",
    "\n",
    "### Objectives\n",
    "\n",
    "* To gauge overall customer sentiment towards KPLC's services.·   \n",
    "\n",
    "* To Identify specific issues mentioned in the tweets, such as token problems, power outages, billing issues, etc.\n",
    "\n",
    "* To Create a chatbot that provides appropriate responses to customer inquiries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "1. Data Collection and Preprocessing:\n",
    "Gathering relevant tweets mentioning KPLC, especially when customers use various hashtags, misspellings or slang, can be difficult. Additionally, cleaning and preprocessing the data (e.g., removing noise like unrelated tweets, abbreviations) is crucial but time-consuming.\n",
    "\n",
    "2. Sentiment Analysis Accuracy:\n",
    "Accurately classifying the sentiment of tweets can be challenging due to the informal language, sarcasm, mixed sentiments and local dialects often used on X/Twitter.\n",
    "\n",
    "3. Identifying Specific Issues:\n",
    "Extracting and categorizing specific issues (e.g power outages, billing issues) mentioned in tweets can be complex due to the diverse ways in which customers describe their problems.\n",
    "\n",
    "4. Real-time Data Processing:\n",
    "Processing a continuous stream of tweets in real-time to provide timely insights and responses is demanding in terms of computational resources and model efficiency.\n",
    "\n",
    "5. Handling Multilingual and Local Dialects:\n",
    "Tweets may be in multiple languages or include local dialects, which can complicate sentiment analysis and issue detection. \n",
    "6. Evaluating Model Performance:\n",
    "Ensuring the models perform well across different contexts, languages, and over time requires ongoing evaluation and tuning.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Proposed Solution\n",
    "\n",
    "* Use advanced Natural Language Processing (NLP) techniques and APIs (e.g., Twitter API) to collect and preprocess tweets.\n",
    "\n",
    "* Implement data cleaning scripts to filter out irrelevant data and normalize the text for consistent analysis. \n",
    "\n",
    "* Train sentiment analysis models using machine learning techniques such as supervised learning with labeled datasets\n",
    "\n",
    "* Implement a robust pipeline using tools for real-time data streaming and processing. Integrate with scalable cloud services such as AWS or Google Cloud to ensure the system can handle large volumes of data efficiently.\n",
    "\n",
    "* Utilize existing chatbot frameworks like Rasa, integrated with the sentiment analysis and issue categorization models. This chatbot should be able to provide relevant responses based on the sentiment and identified issues and direct users to appropriate resources or support channels.\n",
    "\n",
    "* Incorporate multilingual NLP models and fine-tune them with local dialect data. Using translation APIs where necessary to standardize inputs before analysis.\n",
    "\n",
    "* Set up a continuous evaluation framework using A/B testing, cross-validation and performance metrics such as accuracy, F1-score and precision/recall. Regularly retrain models with new data to adapt to evolving customer language and sentiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Metrics of success:\n",
    "\n",
    "* Sentiment Accuracy: Percentage of correctly classified sentiments (positive, negative, neutral).\n",
    "\n",
    "* Issue Detection Rate: Number of key issues identified and addressed based on sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The analysis of the tweets reveals that for Kenya Power and Lightning Company(KPLC),sentiment analysis of the tweets can o along way in assisting the company to understand and deal with customer feedback.In this way,KPLC will be able to focus on identifying the main problems developing and implementing corresponding strategies for the company’s service improvement and ultimately increasing the customer satisfaction level of their customers .The company will be able to maintain their brand image and identify the impending issues before they happen.\n",
    "\n",
    "Despite the difficulties like dealing with  vast data and identification while analyzing the social media concerns ,performing sentiment analysis by analyzing tweets is effective.Since KLC has established key performance indicators of some of its goals such as raise in customer satisfaction scores and positive trend on brand sentiment,the company can use this tool to sustain its leadership in the energy sector while at the same time strengthening its relations with customers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing all the necessary Modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from langdetect import detect\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging all CSV files into one CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path of CSV files\n",
    "path = r'C:\\Users\\USER\\Desktop\\Data\\PHASE5\\KPLC'  # Replace with your actual path\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "# Read and combine all CSV files\n",
    "df_list = [pd.read_csv(file) for file in all_files]\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file called kplc_df.csv\n",
    "combined_df.to_csv('kplc_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have merged all our csvs into one csv file called kplc_df, let us not look at the basic info of our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Information:\n",
      "Shape of the DataFrame: (24774, 15)\n",
      "Data types of each column:\n",
      "Author           object\n",
      "Handle           object\n",
      "Post             object\n",
      "Date             object\n",
      "Likes           float64\n",
      "Reposts         float64\n",
      "Comments        float64\n",
      "Post Link        object\n",
      "Profile Link     object\n",
      "Media URL        object\n",
      "Views            object\n",
      "Name             object\n",
      "Retweets        float64\n",
      "Tweet URL        object\n",
      "Post Body        object\n",
      "dtype: object\n",
      "Missing values per feature:\n",
      "Author            364\n",
      "Handle              0\n",
      "Post              326\n",
      "Date                0\n",
      "Likes           20997\n",
      "Reposts         23305\n",
      "Comments         9311\n",
      "Post Link       14631\n",
      "Profile Link    14305\n",
      "Media URL       24750\n",
      "Views           23572\n",
      "Name            24448\n",
      "Retweets        24763\n",
      "Tweet URL       24448\n",
      "Post Body       24448\n",
      "dtype: int64\n",
      "Removed columns: ['Author', 'Likes', 'Reposts', 'Comments', 'Post Link', 'Views', 'Post Link', 'Profile Link', 'Post Body', 'Retweets', 'Tweet URL', 'Name', 'Media URL']\n",
      "Updated DataFrame:\n",
      "           Handle                                               Post    Date\n",
      "0      @oleykan69  @KenyaPower_Care how long does it take to rest...  29-Apr\n",
      "1       @kak_yeah  Your transformer vihiga county, sabatia subcou...  29-Apr\n",
      "2  @Woud_Nyathira  No light for 28days now and your service men w...  29-Apr\n",
      "3     @ellemanani  @KenyaPower_Care it’s been 26+ hours without p...  29-Apr\n",
      "4       @Smithkmn  When will you guys ever think of doing a phase...  29-Apr\n",
      "Removed 4021 duplicate rows.\n",
      "New shape of the DataFrame: (20753, 3)\n"
     ]
    }
   ],
   "source": [
    "class DataInfo:\n",
    "    def __init__(self, file_path):\n",
    "        # Initialize by reading the CSV file into a DataFrame\n",
    "        self.df = pd.read_csv(file_path)\n",
    "    \n",
    "    def get_shape(self):\n",
    "        # Return the shape of the DataFrame\n",
    "        shape = self.df.shape\n",
    "        print(f\"Shape of the DataFrame: {shape}\")\n",
    "        return shape\n",
    "    \n",
    "    def get_dtypes(self):\n",
    "        # Return the data types of each column\n",
    "        dtypes = self.df.dtypes\n",
    "        print(\"Data types of each column:\")\n",
    "        print(dtypes)\n",
    "        return dtypes\n",
    "    \n",
    "    def get_missing_values(self):\n",
    "        # Return the number of missing values per column\n",
    "        missing_values = self.df.isnull().sum()\n",
    "        print(\"Missing values per feature:\")\n",
    "        print(missing_values)\n",
    "        return missing_values\n",
    "    \n",
    "    def get_basic_info(self):\n",
    "        # Print basic info including shape, data types, and missing values\n",
    "        print(\"Basic Information:\")\n",
    "        self.get_shape()\n",
    "        self.get_dtypes()\n",
    "        self.get_missing_values()\n",
    "    \n",
    "    def remove_irrelevant_columns(self, columns_to_remove):\n",
    "        # Remove only the columns that exist in the DataFrame\n",
    "        existing_columns = [col for col in columns_to_remove if col in self.df.columns]\n",
    "        self.df.drop(columns=existing_columns, inplace=True)\n",
    "        print(f\"Removed columns: {existing_columns}\")\n",
    "        print(\"Updated DataFrame:\")\n",
    "        print(self.df.head())\n",
    "        return self.df\n",
    "    \n",
    "    def remove_duplicates(self):\n",
    "        # Remove duplicate entries based on the 'post' column\n",
    "        initial_shape = self.df.shape\n",
    "        self.df.drop_duplicates(subset='Post', inplace=True)\n",
    "        final_shape = self.df.shape\n",
    "        print(f\"Removed {initial_shape[0] - final_shape[0]} duplicate rows.\")\n",
    "        print(f\"New shape of the DataFrame: {final_shape}\")\n",
    "        return self.df\n",
    "\n",
    "# Instantiating our class\n",
    "data_info = DataInfo(\"kplc_df.csv\")\n",
    "\n",
    "# Get basic information about the dataset\n",
    "data_info.get_basic_info()\n",
    "\n",
    "# Remove irrelevant columns\n",
    "irrelevant_columns = ['Author', 'Likes', 'Reposts', 'Comments', 'Post Link', 'Profile Links', 'Views', 'Post Link', 'Profile Link', 'Post Body', 'Retweets', 'Tweet URL', 'Timestamp', 'Name', 'Media URL', 'Profile Lİnk']\n",
    "cleaned_df = data_info.remove_irrelevant_columns(irrelevant_columns)\n",
    "\n",
    "# Remove duplicates in the 'post' column\n",
    "cleaned_df_no_duplicates = data_info.remove_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our kplc_df dataset has 15 features and some of the features have missing values like likes, Reposts and comments. We have opted for deletion of these columns since they are irrelevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also had different data Types such us floats and Objects. We then checked for duplicates and removed all the duplicated text on post columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us again see how our data looks like and whether there is any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20753 entries, 0 to 24773\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Handle  20753 non-null  object\n",
      " 1   Post    20752 non-null  object\n",
      " 2   Date    20753 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 648.5+ KB\n"
     ]
    }
   ],
   "source": [
    "cleaned_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing there are no null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed to the next stage where we are going to remove any emojis present in our dataset, remove punctuation, lowercase all the posts, remove any noise such as mentions and hashtags, filter-out non-english words, lemmitize and then tokenize our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Handle                                               Post    Date\n",
      "0      @oleykan69  kenyapowercare long take restore transformer d...  29-Apr\n",
      "1       @kak_yeah  transformer vihiga county sabatia subcountysab...  29-Apr\n",
      "2  @Woud_Nyathira  light days service men want bribed ksh k resto...  29-Apr\n",
      "3     @ellemanani  kenyapowercare hours without power transformer...  29-Apr\n",
      "4       @Smithkmn  guys ever think phase balancing transformer ga...  29-Apr\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self, df, text_column):\n",
    "        \"\"\"\n",
    "        Initialize the TextCleaner class with a DataFrame and the text column to clean.\n",
    "        \n",
    "        :param df: DataFrame containing the data\n",
    "        :param text_column: The name of the column to clean\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.text_column = text_column\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def handle_emojis(self, text):\n",
    "        \"\"\"Replace emojis with corresponding text descriptions.\"\"\"\n",
    "        return emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    \n",
    "    def to_lowercase(self, text):\n",
    "        \"\"\"Convert text to lowercase.\"\"\"\n",
    "        return text.lower()\n",
    "    \n",
    "    def remove_punctuation_numbers(self, text):\n",
    "        \"\"\"Remove punctuation and numbers from the text.\"\"\"\n",
    "        return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    def remove_mentions_hashtags(self, text):\n",
    "        \"\"\"Remove mentions (@) and hashtags (#) from the text.\"\"\"\n",
    "        return re.sub(r'[@#]\\w+', '', text)\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"Remove stopwords from the text.\"\"\"\n",
    "        return ' '.join(word for word in text.split() if word not in self.stop_words)\n",
    "    \n",
    "    def clean_single_text(self, text):\n",
    "        \"\"\"Clean a single piece of text by applying all steps.\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return ''\n",
    "        text = self.handle_emojis(text)\n",
    "        text = self.to_lowercase(text)\n",
    "        text = self.remove_punctuation_numbers(text)\n",
    "        text = self.remove_mentions_hashtags(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        return text\n",
    "    \n",
    "    def clean_text(self):\n",
    "        \"\"\"Apply all cleaning steps to the specified text column in the DataFrame.\"\"\"\n",
    "        self.df[self.text_column] = self.df[self.text_column].apply(lambda text: self.clean_single_text(text))\n",
    "        return self.df\n",
    "\n",
    "# Instantiate our TextCleaner Class\n",
    "text_cleaner = TextCleaner(cleaned_df, text_column='Post')\n",
    "cleaned_df = text_cleaner.clean_text()\n",
    "cleaned_df.to_csv('final_cleaned_kplc_df.csv', index=False)\n",
    "print(cleaned_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
