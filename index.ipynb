{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUSINESS UNDERSTANDING\n",
    "\n",
    "Overview\n",
    "Kenya Power and Lighting Company (KPLC) often receives a high volume of tweets from customers reporting issues, asking questions, or providing feedback.Understanding customer sentiment towards KPLC is crucial to enable automating of responses, enhancing customer service efficiency, improving response times, and reduce the manual workload on customer service teams. The goal is to develop a chatbot capable of classifying various types of tweets and generating appropriate automated responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "KPLC needs an automated sentiment analysis system to process and categorize customer feedback from social media, particularly X formerly (Twitter) where customers frequently express their sentiments regarding KPLC's services. By accurately classifying tweets related to KPLC’s services into sentiment categories the system will be able to identify issues by pinpointing common complaints and service issues and enhance customer feedback\n",
    "\n",
    "### Objectives\n",
    "\n",
    "* To gauge overall customer sentiment towards KPLC's services.·   \n",
    "\n",
    "* To Identify specific issues mentioned in the tweets, such as token problems, power outages, billing issues, etc.\n",
    "\n",
    "* To Create a chatbot that provides appropriate responses to customer inquiries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "1. Data Collection and Preprocessing:\n",
    "Gathering relevant tweets mentioning KPLC, especially when customers use various hashtags, misspellings or slang, can be difficult. Additionally, cleaning and preprocessing the data (e.g., removing noise like unrelated tweets, abbreviations) is crucial but time-consuming.\n",
    "\n",
    "2. Sentiment Analysis Accuracy:\n",
    "Accurately classifying the sentiment of tweets can be challenging due to the informal language, sarcasm, mixed sentiments and local dialects often used on X/Twitter.\n",
    "\n",
    "3. Identifying Specific Issues:\n",
    "Extracting and categorizing specific issues (e.g power outages, billing issues) mentioned in tweets can be complex due to the diverse ways in which customers describe their problems.\n",
    "\n",
    "4. Real-time Data Processing:\n",
    "Processing a continuous stream of tweets in real-time to provide timely insights and responses is demanding in terms of computational resources and model efficiency.\n",
    "\n",
    "5. Handling Multilingual and Local Dialects:\n",
    "Tweets may be in multiple languages or include local dialects, which can complicate sentiment analysis and issue detection. \n",
    "6. Evaluating Model Performance:\n",
    "Ensuring the models perform well across different contexts, languages, and over time requires ongoing evaluation and tuning.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Proposed Solution\n",
    "\n",
    "* Use advanced Natural Language Processing (NLP) techniques and APIs (e.g., Twitter API) to collect and preprocess tweets.\n",
    "\n",
    "* Implement data cleaning scripts to filter out irrelevant data and normalize the text for consistent analysis. \n",
    "\n",
    "* Train sentiment analysis models using machine learning techniques such as supervised learning with labeled datasets\n",
    "\n",
    "* Implement a robust pipeline using tools for real-time data streaming and processing. Integrate with scalable cloud services such as AWS or Google Cloud to ensure the system can handle large volumes of data efficiently.\n",
    "\n",
    "* Utilize existing chatbot frameworks like Rasa, integrated with the sentiment analysis and issue categorization models. This chatbot should be able to provide relevant responses based on the sentiment and identified issues and direct users to appropriate resources or support channels.\n",
    "\n",
    "* Incorporate multilingual NLP models and fine-tune them with local dialect data. Using translation APIs where necessary to standardize inputs before analysis.\n",
    "\n",
    "* Set up a continuous evaluation framework using A/B testing, cross-validation and performance metrics such as accuracy, F1-score and precision/recall. Regularly retrain models with new data to adapt to evolving customer language and sentiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Metrics of success:\n",
    "\n",
    "* Sentiment Accuracy: Percentage of correctly classified sentiments (positive, negative, neutral).\n",
    "\n",
    "* Issue Detection Rate: Number of key issues identified and addressed based on sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The analysis of the tweets reveals that for Kenya Power and Lightning Company(KPLC),sentiment analysis of the tweets can o along way in assisting the company to understand and deal with customer feedback.In this way,KPLC will be able to focus on identifying the main problems developing and implementing corresponding strategies for the company’s service improvement and ultimately increasing the customer satisfaction level of their customers .The company will be able to maintain their brand image and identify the impending issues before they happen.\n",
    "\n",
    "Despite the difficulties like dealing with  vast data and identification while analyzing the social media concerns ,performing sentiment analysis by analyzing tweets is effective.Since KLC has established key performance indicators of some of its goals such as raise in customer satisfaction scores and positive trend on brand sentiment,the company can use this tool to sustain its leadership in the energy sector while at the same time strengthening its relations with customers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing all the necessary Modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from langdetect import detect\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging all CSV files into one CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path of CSV files\n",
    "path = r'C:\\Users\\USER\\Desktop\\Data\\PHASE5\\KPLC'  # Replace with your actual path\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "# Read and combine all CSV files\n",
    "df_list = [pd.read_csv(file) for file in all_files]\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file called kplc_df.csv\n",
    "combined_df.to_csv('kplc_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have merged all our csvs into one csv file called kplc_df, let us not look at the basic info of our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Information:\n",
      "Shape of the DataFrame: (4408, 16)\n",
      "Data types of each column:\n",
      "Author           object\n",
      "Handle           object\n",
      "Media URL        object\n",
      "Reposts         float64\n",
      "Likes           float64\n",
      "Comments        float64\n",
      "Views            object\n",
      "Post Link        object\n",
      "Profile Link     object\n",
      "Post             object\n",
      "Date             object\n",
      "Name             object\n",
      "Retweets        float64\n",
      "Tweet URL        object\n",
      "Post Body        object\n",
      "Timestamp        object\n",
      "dtype: object\n",
      "Missing values per feature:\n",
      "Author           335\n",
      "Handle             0\n",
      "Media URL       4384\n",
      "Reposts         4150\n",
      "Likes           3726\n",
      "Comments        1942\n",
      "Views           3206\n",
      "Post Link        326\n",
      "Profile Link       0\n",
      "Post             326\n",
      "Date               0\n",
      "Name            4082\n",
      "Retweets        4397\n",
      "Tweet URL       4082\n",
      "Post Body       4082\n",
      "Timestamp       4082\n",
      "dtype: int64\n",
      "Removed columns: ['Author', 'Likes', 'Reposts', 'Comments', 'Post Link', 'Views', 'Post Link', 'Profile Link', 'Post Body', 'Retweets', 'Tweet URL']\n",
      "Updated DataFrame:\n",
      "             Handle Media URL  \\\n",
      "0  @Momanyi10908868       NaN   \n",
      "1   @FrancisKimunya       NaN   \n",
      "2        @LinaCheps       NaN   \n",
      "3     @CShihembetsa       NaN   \n",
      "4     @CShihembetsa       NaN   \n",
      "\n",
      "                                                Post    Date Name Timestamp  \n",
      "0  We are already in a blackout as @6pm, 37170880035  29-Jul  NaN       NaN  \n",
      "1  @KenyaPower_Care we are in blackout the whole ...  29-Jul  NaN       NaN  \n",
      "2          We are in a blackout again please resolve  29-Jul  NaN       NaN  \n",
      "3  @KenyaPower_Care @KenyaPower kindly check out ...  28-Jul  NaN       NaN  \n",
      "4  @KenyaPower_Care kindly check out power line, ...  28-Jul  NaN       NaN  \n",
      "Removed 982 duplicate rows.\n",
      "New shape of the DataFrame: (3426, 6)\n"
     ]
    }
   ],
   "source": [
    "class DataInfo:\n",
    "    def __init__(self, file_path):\n",
    "        # Initialize by reading the CSV file into a DataFrame\n",
    "        self.df = pd.read_csv(file_path)\n",
    "    \n",
    "    def get_shape(self):\n",
    "        # Return the shape of the DataFrame\n",
    "        shape = self.df.shape\n",
    "        print(f\"Shape of the DataFrame: {shape}\")\n",
    "        return shape\n",
    "    \n",
    "    def get_dtypes(self):\n",
    "        # Return the data types of each column\n",
    "        dtypes = self.df.dtypes\n",
    "        print(\"Data types of each column:\")\n",
    "        print(dtypes)\n",
    "        return dtypes\n",
    "    \n",
    "    def get_missing_values(self):\n",
    "        # Return the number of missing values per column\n",
    "        missing_values = self.df.isnull().sum()\n",
    "        print(\"Missing values per feature:\")\n",
    "        print(missing_values)\n",
    "        return missing_values\n",
    "    \n",
    "    def get_basic_info(self):\n",
    "        # Print basic info including shape, data types, and missing values\n",
    "        print(\"Basic Information:\")\n",
    "        self.get_shape()\n",
    "        self.get_dtypes()\n",
    "        self.get_missing_values()\n",
    "    \n",
    "    def remove_irrelevant_columns(self, columns_to_remove):\n",
    "        # Remove only the columns that exist in the DataFrame\n",
    "        existing_columns = [col for col in columns_to_remove if col in self.df.columns]\n",
    "        self.df.drop(columns=existing_columns, inplace=True)\n",
    "        print(f\"Removed columns: {existing_columns}\")\n",
    "        print(\"Updated DataFrame:\")\n",
    "        print(self.df.head())\n",
    "        return self.df\n",
    "    \n",
    "    def remove_duplicates(self):\n",
    "        # Remove duplicate entries based on the 'post' column\n",
    "        initial_shape = self.df.shape\n",
    "        self.df.drop_duplicates(subset='Post', inplace=True)\n",
    "        final_shape = self.df.shape\n",
    "        print(f\"Removed {initial_shape[0] - final_shape[0]} duplicate rows.\")\n",
    "        print(f\"New shape of the DataFrame: {final_shape}\")\n",
    "        return self.df\n",
    "\n",
    "# Instantiating our class\n",
    "data_info = DataInfo(\"kplc_df.csv\")\n",
    "\n",
    "# Get basic information about the dataset\n",
    "data_info.get_basic_info()\n",
    "\n",
    "# Remove irrelevant columns\n",
    "irrelevant_columns = ['Author', 'Likes', 'Reposts', 'Comments', 'Post Link', 'Profile Links', 'Views', 'Post Link', 'Profile Link', 'Post Body', 'Retweets', 'Tweet URL']\n",
    "cleaned_df = data_info.remove_irrelevant_columns(irrelevant_columns)\n",
    "\n",
    "# Remove duplicates in the 'post' column\n",
    "cleaned_df_no_duplicates = data_info.remove_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our kplc_df dataset has 9 features and some of the features have missing values like likes, Reposts and comments. We have opted for deletion of these columns since they are irrelevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also had different data Types such us floats and Objects. We then checked for duplicates and removed all the duplicated text on post columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us again see how our data looks like and whether there is any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3426 entries, 0 to 4407\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Handle     3426 non-null   object\n",
      " 1   Media URL  12 non-null     object\n",
      " 2   Post       3425 non-null   object\n",
      " 3   Date       3426 non-null   object\n",
      " 4   Name       1 non-null      object\n",
      " 5   Timestamp  1 non-null      object\n",
      "dtypes: object(6)\n",
      "memory usage: 187.4+ KB\n"
     ]
    }
   ],
   "source": [
    "cleaned_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing there are no null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed to the next stage where we are going to remove any emojis present in our dataset, remove punctuation, lowercase all the posts, remove any noise such as mentions and hashtags, filter-out non-english words, lemmitize and then tokenize our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Handle Media URL  \\\n",
      "0  @Momanyi10908868       NaN   \n",
      "1   @FrancisKimunya       NaN   \n",
      "2        @LinaCheps       NaN   \n",
      "3     @CShihembetsa       NaN   \n",
      "4     @CShihembetsa       NaN   \n",
      "\n",
      "                                                Post    Date Name Timestamp  \n",
      "0                  we are already in a blackout a pm  29-Jul  NaN       NaN  \n",
      "1  kenyapowercare we are in blackout the whole of...  29-Jul  NaN       NaN  \n",
      "2          we are in a blackout again please resolve  29-Jul  NaN       NaN  \n",
      "3  kenyapowercare kenyapower kindly check out pow...  28-Jul  NaN       NaN  \n",
      "4  kenyapowercare kindly check out power line wev...  28-Jul  NaN       NaN  \n"
     ]
    }
   ],
   "source": [
    "class TextCleaner:\n",
    "    def __init__(self, df, text_column):\n",
    "        \"\"\"\n",
    "        Initialize the TextCleaner class with a DataFrame and the text column to clean.\n",
    "        \n",
    "        :param df: DataFrame containing the data\n",
    "        :param text_column: The name of the column to clean\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.text_column = text_column\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def handle_emojis(self, text):\n",
    "        \"\"\"Replace emojis with corresponding text descriptions.\"\"\"\n",
    "        return emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    \n",
    "    def to_lowercase(self, text):\n",
    "        \"\"\"Convert text to lowercase.\"\"\"\n",
    "        return text.lower()\n",
    "    \n",
    "    def remove_punctuation_numbers(self, text):\n",
    "        \"\"\"Remove punctuation and numbers from the text.\"\"\"\n",
    "        return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    def remove_mentions_hashtags(self, text):\n",
    "        \"\"\"Remove mentions (@) and hashtags (#) from the text.\"\"\"\n",
    "        return re.sub(r'[@#]\\w+', '', text)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize the text into words.\"\"\"\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    def lemmatize(self, tokens):\n",
    "        \"\"\"Lemmatize the tokens.\"\"\"\n",
    "        return [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    def filter_non_english(self, text):\n",
    "        \"\"\"Filter out text that is not in English.\"\"\"\n",
    "        try:\n",
    "            return text if detect(text) == 'en' else ''\n",
    "        except:\n",
    "            return ''\n",
    "    \n",
    "    def clean_text(self):\n",
    "        \"\"\"Apply all cleaning steps to the specified text column in the DataFrame.\"\"\"\n",
    "        self.df[self.text_column] = self.df[self.text_column].apply(lambda text: self.clean_single_text(text))\n",
    "        return self.df\n",
    "    \n",
    "    def clean_single_text(self, text):\n",
    "        \"\"\"Clean a single piece of text by applying all steps.\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return ''\n",
    "        text = self.handle_emojis(text)\n",
    "        text = self.to_lowercase(text)\n",
    "        text = self.remove_punctuation_numbers(text)\n",
    "        text = self.remove_mentions_hashtags(text)\n",
    "        text = self.filter_non_english(text)\n",
    "        if text:\n",
    "            tokens = self.tokenize(text)\n",
    "            lemmatized_tokens = self.lemmatize(tokens)\n",
    "            return ' '.join(lemmatized_tokens)\n",
    "        return ''\n",
    "\n",
    "\n",
    "# Create an instance of TextCleaner for the 'post' column\n",
    "text_cleaner = TextCleaner(cleaned_df, text_column='Post')\n",
    "\n",
    "# Clean the text\n",
    "cleaned_df = text_cleaner.clean_text()\n",
    "\n",
    "# Optionally, save the cleaned DataFrame to a new CSV file\n",
    "cleaned_df.to_csv('final_cleaned_kplc_df.csv', index=False)\n",
    "\n",
    "# Return the cleaned DataFrame\n",
    "print(cleaned_df.head())  # Display the first few rows to verify"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
